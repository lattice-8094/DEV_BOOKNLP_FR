Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/data/booknlp/JEAN_DEV_BOOKNLP_FR/booknlp_env/lib/python3.9/site-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op
  warnings.warn("Initializing zero-element tensors is a no-op")
/data/booknlp/JEAN_DEV_BOOKNLP_FR/training/layered_reader.py:310: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  batched_transforms.append(torch.FloatTensor(batch_transforms))

********************************************
Running on: cuda
********************************************

{'mode': 'test', 'batch_prediction_file': None, 'input_prediction_file': None, 'output_prediction_file': None, 'trainFolder_flat': None, 'testFolder_flat': None, 'devFolder_flat': None, 'trainFolder_layered': '/data/booknlp/JEAN_DEV_BOOKNLP_FR/training/TRAINING_DATA/ENTITIES/sacr_archive/train', 'testFolder_layered': '/data/booknlp/JEAN_DEV_BOOKNLP_FR/training/TRAINING_DATA/ENTITIES/sacr_archive/test', 'devFolder_layered': '/data/booknlp/JEAN_DEV_BOOKNLP_FR/training/TRAINING_DATA/ENTITIES/sacr_archive/dev', 'trainFolder_supersense': None, 'testFolder_supersense': None, 'devFolder_supersense': None, 'tagFile_flat': 'files/event.tagset', 'tagFile_layered': '/data/booknlp/JEAN_DEV_BOOKNLP_FR/training/TRAINING_DATA/ENTITIES/sacr_archive/fr.entities.tagset', 'tagFile_supersense': 'files/supersense.tagset', 'modelFile': 'TRAINED_MODELS/ALL_CAT/VARCHIVE/fr_catprop_camembert-base.model', 'flat_metric': None, 'base_model': 'camembert-base', 'ignoreEvents': False, 'ignoreEntities': False}
MODEL camembert-base
test.data
num sentences: 1400
precision: 0.805 4236.0/5261
recall: 0.864 4236.0/4901
F: 0.834

	PRON precision: 0.844 2684.0/3181
	PRON recall: 0.919 2684.0/2920
	PRON F: 0.880

	NOM precision: 0.724 1243.0/1716
	NOM recall: 0.783 1243.0/1587
	NOM F: 0.753

	PROP precision: 0.849 309.0/364
	PROP recall: 0.784 309.0/394
	PROP F: 0.815

	DISCUSS precision: 0.000 0.0/0
	DISCUSS recall: 0.000 0.0/0
	DISCUSS F: 0.000

	FAC precision: 0.693 181.0/261
	FAC recall: 0.733 181.0/247
	FAC F: 0.713

	GPE precision: 0.645 40.0/62
	GPE recall: 0.471 40.0/85
	GPE F: 0.544

	HIST precision: 0.000 0.0/6
	HIST recall: 0.000 0.0/2
	HIST F: 0.000

	LOC precision: 0.640 48.0/75
	LOC recall: 0.414 48.0/116
	LOC F: 0.503

	METALEPSE precision: 0.000 0.0/4
	METALEPSE recall: 0.000 0.0/0
	METALEPSE F: 0.000

	ORG precision: 0.000 0.0/1
	ORG recall: 0.000 0.0/7
	ORG F: 0.000

	PER precision: 0.825 3761.0/4559
	PER recall: 0.937 3761.0/4016
	PER F: 0.877

	TIME precision: 0.741 163.0/220
	TIME recall: 0.453 163.0/360
	TIME F: 0.562

	VEH precision: 0.589 43.0/73
	VEH recall: 0.652 43.0/66
	VEH F: 0.619

	X precision: 0.000 0.0/0
	X recall: 0.000 0.0/0
	X F: 0.000
